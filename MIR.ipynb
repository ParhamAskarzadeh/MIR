{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " پرهام عسکرزاده\n",
    " شماره دانشجویی : ۹۸۱۷۰۹۳۵\n",
    "\n",
    "در این تمرین ما متن های مورد نظر بورسی را از سایت رهاورد ۳۶۵ جمع میکنیم و بر روی مراحل پیش پردازش متن را اعمال میکنیم و در آخر گزارشی از اطلاعات منن و همچنین تشخیص میدهیم که متن مورد نظر راجع کدام سهام بورسی به اطلاعات میدهد"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این بخش پکیج های مورد نظر برای کرول کردن اطلاعات سایت رهاورد ۳۶۵ را اضافه میکنیم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from requests import ReadTimeout, TooManyRedirects, Timeout\n",
    "from requests.exceptions import ChunkedEncodingError, ConnectionError\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " متن های مورد نظر ما از بخش گفتگو های سایت رهاورد ۳۶۵ کرول و جمع آوری میشوند\n",
    " قطعه کد زیر مربوط به کرولر تمرین است:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class TweetCrawler:\n",
    "\n",
    "    def crawl_tweets(self, before_id=999999):\n",
    "        url = f'https://rahavard365.com/api/moreposts?before_id={before_id}'\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',\n",
    "            'sec-fetch-site': 'same-origin',\n",
    "            'sec-fetch-mode': 'cors',\n",
    "            'sec-fetch-dest': 'empty'\n",
    "        }\n",
    "        while True:\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=25)\n",
    "                if response.status_code == 429:\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "            except (Timeout, ReadTimeout, ConnectionError, TooManyRedirects, ChunkedEncodingError) as e:\n",
    "                time.sleep(30)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "            result = json.loads(response.text)\n",
    "            return result['data'].get('posts')[random.randint(0, 19)]['body']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " با اجرا کردن این متد متن مورد نظر جمع شده و به صورت رندوم یکی از آنها نمایش داده می شود"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "'سلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TweetCrawler().crawl_tweets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " حال به بخش نرمالایز کردن متن می رسیم در این بخش علاوه بر استفاده از پکیج هضم از متد های تعریف شده ای که کار نرمال کردن و یکسان کردن متن و همجنین حذف stopowrd ها و emoji ها استفاده میکنیم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/nltk/parse/malt.py:183: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if ret is not 0:\n",
      "/usr/lib/python3.10/site-packages/nltk/ccg/chart.py:274: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if len(children) is 2:\n"
     ]
    }
   ],
   "source": [
    "from hazm import Normalizer\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class SelfNormalizer:\n",
    "    def __init__(self):\n",
    "        self.normalizer = Normalizer()\n",
    "\n",
    "    def remove_shapes_and_convert_emojis_to_unicode(self, text: str):\n",
    "        shape_list = re.findall(r'[^\\w\\s,]', text)  # find all shape and emojis\n",
    "        for shape in shape_list:\n",
    "            shape_code = shape.encode('unicode-escape').decode('ASCII')\n",
    "            if 'U000' in shape_code:  # if the shape is an emoji\n",
    "                text = text.replace(shape, ' {} '.format(shape_code))\n",
    "            else:\n",
    "                text = text.replace(shape, ' ')\n",
    "        return text\n",
    "\n",
    "    def character_replacer(self, text: str):\n",
    "        # Numbers\n",
    "        text = re.sub(r'[٠⓪⓿０𝟶🄌]', '۰', text)\n",
    "        text = re.sub(r'[١⓵❶➀➊꘡]', '۱', text)\n",
    "        text = re.sub(r'[٢②２𝟐]', '۲', text)\n",
    "        text = re.sub(r'[٣③３𝟛]', '۳', text)\n",
    "        text = re.sub(r'[٤۴⓸➍𝟒𝟜]', '۴', text)\n",
    "        text = re.sub(r'[٥⓹❺５𝟝]', '۵', text)\n",
    "        text = re.sub(r'[٦۶⑥❻６𝟞𝟨]', '۶', text)\n",
    "        text = re.sub(r'[٧➆➐７𝟟]', '۷', text)\n",
    "        text = re.sub(r'[٨⑧❽８𝟖]', '۸', text)\n",
    "        text = re.sub(r'[٩⑨❾𝟗]', '۹', text)\n",
    "\n",
    "        # Alphabet\n",
    "        text = re.sub(r'[آأ𞸀]', 'ا', text)\n",
    "        text = re.sub(r'[بﭒﭓﭔﭕ𞸁]', 'ب', text)\n",
    "        text = re.sub(r'[ﭗﭘﭙ]', 'پ', text)\n",
    "        text = re.sub(r'[تﺖﭧ𞸕]', 'ت', text)\n",
    "        text = re.sub(r'[ثﺙﺚ𞸶𞸖]', 'ث', text)\n",
    "        text = re.sub(r'[ﺝﺞﺠ𞸢𞸂]', 'ج', text)\n",
    "        text = re.sub(r'[چﭻﭼﮀ]', 'چ', text)\n",
    "        text = re.sub(r'[حﺢﺣ𞸇]', 'ح', text)\n",
    "        text = re.sub(r'[ﺦﺨ𞸗]', 'خ', text)\n",
    "\n",
    "        text = re.sub(r'[دﺩﺪ]', 'د', text)\n",
    "        text = re.sub(r'[ذﺫﺬ𞸘]', 'ذ', text)\n",
    "\n",
    "        text = re.sub(r'[رﺭﺮ𞸓]', 'ر', text)\n",
    "        text = re.sub(r'[زࢲﺯﺰ𞸆]', 'ز', text)\n",
    "        text = re.sub(r'[ژﮊﮋ]', 'ژ', text)\n",
    "        text = re.sub(r'[ﺱﺳﺴ𞸎𞸮]', 'س', text)\n",
    "        text = re.sub(r'[ﺵﺶﺸ𞸴𞸔]', 'ش', text)\n",
    "        text = re.sub(r'[ص𞸱𞸑]', 'ص', text)\n",
    "        text = re.sub(r'[ضﻀ𞸹𞸙]', 'ض', text)\n",
    "        text = re.sub(r'[ﻂﻃ𞸈]', 'ط', text)\n",
    "        text = re.sub(r'[ﻆ𞸚]', 'ظ', text)\n",
    "        text = re.sub(r'[عﻉﻊﻌ𞸯𞸏]', 'ع', text)\n",
    "        text = re.sub(r'[ﻎﻏﻐ𞸻𞸛]', 'غ', text)\n",
    "        text = re.sub(r'[ف𞸞𞸐]', 'ف', text)\n",
    "        text = re.sub(r'[ﻖﻘ𞸟𞸒]', 'ق', text)\n",
    "        text = re.sub(r'[گﮓﮔﮕ]', 'گ', text)\n",
    "        text = re.sub(r'[كﮑ𞸊𞸪]', 'ک', text)\n",
    "        text = re.sub(r'[ﻝﻞﻟ𞸋]', 'ل', text)\n",
    "        text = re.sub(r'[مﻡﻤ𞸬𞸌]', 'م', text)\n",
    "        text = re.sub(r'[ﻥ𞸍𞸭]', 'ن', text)\n",
    "        text = re.sub(r'[ﻭﻮ𞸅ۅﯠ]', 'و', text)\n",
    "        text = text.replace('وو', 'و')\n",
    "        text = re.sub(r'[هﮪﻪ𞸤ﻫﻬ]', 'ه', text)\n",
    "        text = re.sub(r'[ةﺔ]', 'ه', text)\n",
    "        text = re.sub(r'[ىﻯﻰ𞸉ﯨﯩ]', 'ی', text)\n",
    "        text = text.replace('ئی', 'یی')\n",
    "        text = re.sub(r'[ئﺉﺋ]', 'ئ', text)\n",
    "        text = re.sub(r'[ءﺀ۽]', 'ء', text)\n",
    "        text = text.replace('﷼', ' ریال ')\n",
    "\n",
    "        text = text.replace(' می ', ' می\\u200c')\n",
    "        text = text.replace(' نمی ', ' نمی\\u200c')\n",
    "        text = text.replace(' برمی ', ' برمی\\u200c')\n",
    "        text = text.replace(' برنمی ', ' برنمی\\u200c')\n",
    "\n",
    "        # Whitespace\n",
    "        text = re.sub(r'(\\r|\\f|\\v|\\\\r|\\\\n)+', '\\n', text)\n",
    "        text = re.sub(r'\\s?\\n\\s+', '\\n', text)\n",
    "        text = re.sub(r'[\\t]+', ' ', text)\n",
    "        text = re.sub(r' {2,}', ' ', text)\n",
    "\n",
    "        # semi-space\n",
    "        text = text.replace('&zwnj;', '\\u200c')\n",
    "        text = re.sub(r'[\\u2000-\\u200f]+', \"\\u200c\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_stopword(self, text: str):\n",
    "        stop_words = ['از', 'به ', 'با', ' نه ', 'را', 'که ', 'بود', 'است', 'هست', 'شد', ' در ', 'اگر ', 'همچنین ',\n",
    "                      'چنین ', 'داشت']\n",
    "        for word in stop_words:\n",
    "            text = re.sub(word, ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalize_with_hazm(self, text: str):\n",
    "        return self.normalizer.normalize(text)\n",
    "\n",
    "    def complete_normalize(self, text: str) -> str:\n",
    "        text = self.remove_shapes_and_convert_emojis_to_unicode(text)\n",
    "        text = self.character_replacer(text)\n",
    "        text = self.normalizer.normalize(text)\n",
    "        text = self.remove_stopword(text)\n",
    "\n",
    "        return text.strip()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این بخش اموجی ها و اشکال که کاربردی برای ما ندارند از متن مورد نظر حذف می شوند"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'سلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelfNormalizer().remove_shapes_and_convert_emojis_to_unicode()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " این متد همان کار نرمال کردن را انجام میدهد که بدون استفاده از هرگونه پکیجی نوشته شده"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'سلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelfNormalizer().character_replacer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این متد stopword های مورد نظر از متن حذف میشوند"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'سلام دوستان من امروز ورود کردم نظزتون رو صادقا بگید'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelfNormalizer().remove_stopword()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این بخش با استفاده از پکیج هضم متن را نرمالایز میکنیم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'سلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelfNormalizer().normalize_with_hazm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " این متد تجمیعی از متد های بالاست که متن را میگیرد و عمل های فوق را روی متن مورد نظر انجام میده"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'سلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelfNormalizer().complete_normalize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " بعد از نرمال کردن متن نوبت به گرفتن اطلاعات از متن نرمال شده میباشد\n",
    " ما در این کلاس اطلاعاتی قبیل : منشن ها و تعداد آنها ، هشتگ ها و تعداد آنها ، عدد ها و تعداد آنها داخل متن و همچنین نقش کلمات در جمله و همچنین توکن های جمله را خروجی میدهیم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from hazm import POSTagger, SentenceTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    def __init__(self):\n",
    "        self.sign = {\",\", \".\", \"،\", \"?\", \"؟\", \"!\", \"!\", \"#\", \"*\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \" \"}\n",
    "        self.sentence_tokenizer = SentenceTokenizer()\n",
    "        self.pos_tagger = POSTagger()\n",
    "\n",
    "    def mentions(self, text: str):\n",
    "        return list(set(re.findall('@([\\w_.]+)', text)))\n",
    "\n",
    "    def hashtags(self, text: str):\n",
    "        return list(set(re.findall('#([\\w_]+)', text)))\n",
    "\n",
    "    def numbers(self, text: str):\n",
    "        number = ''\n",
    "        numbers = []\n",
    "        for letter in text:\n",
    "            try:\n",
    "                int(letter)\n",
    "            except:\n",
    "                if len(number) != 0:\n",
    "                    numbers.append(number)\n",
    "                    number = ''\n",
    "                continue\n",
    "            number += letter\n",
    "        return numbers\n",
    "\n",
    "    def letter_info(self, text: str):\n",
    "        letters = []\n",
    "        namads = []\n",
    "        for letter in text:\n",
    "            if letter not in self.sign:\n",
    "                letters.append(letter)\n",
    "            elif letter != \" \":\n",
    "                namads.append(letter)\n",
    "        return {\"letter_count\": len(letters),\n",
    "                \"sign_count\": len(namads)}\n",
    "\n",
    "    def info_of_text(self, text: str):\n",
    "        return {\n",
    "                'numbers_info': {\n",
    "                    'numbers': self.numbers(text),\n",
    "                    'count': len(self.numbers(text))},\n",
    "                'letters_count': self.letter_info(text),\n",
    "                'hashtags_info': {\n",
    "                    'items': self.hashtags(text),\n",
    "                    'count': len(self.hashtags(text))},\n",
    "                'mentions_info': {\n",
    "                    'items': self.mentions(text),\n",
    "                    'count': len(self.mentions(text))}\n",
    "                }\n",
    "\n",
    "    def tokenizer(self, text: str):\n",
    "        return self.sentence_tokenizer.tokenize(text)\n",
    "\n",
    "    def postagger(self, text):\n",
    "        return self.pos_tagger.tag(text)\n",
    "\n",
    "    def rate_keywords(self, text):\n",
    "        rate = Counter(text.split())\n",
    "        all = sum(dict(rate).values())\n",
    "        info_rate = {}\n",
    "        for word in rate:\n",
    "            info_rate[word] = round(float((rate[word] / all) * 100), 2)\n",
    "        return info_rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این بخش ما منشن های داخل متن و همچنین تعداد آنها رو خروجی میگیریم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Model.__del__ at 0x7f4acadeda20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/site-packages/wapiti/api.py\", line 289, in __del__\n",
      "    if _wapiti and self._model:\n",
      "AttributeError: 'Model' object has no attribute '_model'\n",
      "Exception ignored in: <function Model.__del__ at 0x7f4acadeda20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/site-packages/wapiti/api.py\", line 289, in __del__\n",
      "    if _wapiti and self._model:\n",
      "AttributeError: 'Model' object has no attribute '_model'\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <class 'TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mArgumentError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [46]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmentions(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mسلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [45]\u001B[0m, in \u001B[0;36mAnalyzer.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msign \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m،\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m؟\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentence_tokenizer \u001B[38;5;241m=\u001B[39m SentenceTokenizer()\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_tagger \u001B[38;5;241m=\u001B[39m \u001B[43mPOSTagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/hazm/SequenceTagger.py:23\u001B[0m, in \u001B[0;36mSequenceTagger.__init__\u001B[0;34m(self, patterns, **options)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, patterns\u001B[38;5;241m=\u001B[39m[], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions):\n\u001B[1;32m     22\u001B[0m \t\u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwapiti\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[0;32m---> 23\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatterns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/wapiti/api.py:283\u001B[0m, in \u001B[0;36mModel.__init__\u001B[0;34m(self, patterns, encoding, **options)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatterns \u001B[38;5;241m=\u001B[39m patterns\n\u001B[0;32m--> 283\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model \u001B[38;5;241m=\u001B[39m \u001B[43m_wapiti\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi_new_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m    \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpointer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatterns\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mArgumentError\u001B[0m: argument 2: <class 'TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "Analyzer().mentions('سلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این بخش ما هشتگ های داخل متن و همچنین تعداد آنها رو خروجی میگیریم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <class 'TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mArgumentError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [30]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mhashtags(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mسلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [28]\u001B[0m, in \u001B[0;36mAnalyzer.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msign \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m،\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m؟\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentence_tokenizer \u001B[38;5;241m=\u001B[39m SentenceTokenizer()\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_tagger \u001B[38;5;241m=\u001B[39m \u001B[43mPOSTagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/hazm/SequenceTagger.py:23\u001B[0m, in \u001B[0;36mSequenceTagger.__init__\u001B[0;34m(self, patterns, **options)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, patterns\u001B[38;5;241m=\u001B[39m[], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions):\n\u001B[1;32m     22\u001B[0m \t\u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwapiti\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[0;32m---> 23\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatterns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/wapiti/api.py:283\u001B[0m, in \u001B[0;36mModel.__init__\u001B[0;34m(self, patterns, encoding, **options)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatterns \u001B[38;5;241m=\u001B[39m patterns\n\u001B[0;32m--> 283\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model \u001B[38;5;241m=\u001B[39m \u001B[43m_wapiti\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi_new_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m    \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpointer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatterns\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mArgumentError\u001B[0m: argument 2: <class 'TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "Analyzer().hashtags()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این بخش ما عدد های داخل متن و همچنین تعداد آنها رو خروجی میگیریم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Analyzer().numbers()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این بخش ما تعداد حروفو نشانه های داخل متن را خروجی میگیریم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Analyzer().letter_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " این بخش اطلاعات متد های بالا رابه صورت تجمیعی و یکجا به کاربر نمایش می دهد"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <class 'TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mArgumentError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [36]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39minfo_of_text(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mسلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [33]\u001B[0m, in \u001B[0;36mAnalyzer.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msign \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m،\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m؟\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentence_tokenizer \u001B[38;5;241m=\u001B[39m SentenceTokenizer()\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_tagger \u001B[38;5;241m=\u001B[39m \u001B[43mPOSTagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/hazm/SequenceTagger.py:23\u001B[0m, in \u001B[0;36mSequenceTagger.__init__\u001B[0;34m(self, patterns, **options)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, patterns\u001B[38;5;241m=\u001B[39m[], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions):\n\u001B[1;32m     22\u001B[0m \t\u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwapiti\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[0;32m---> 23\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatterns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/wapiti/api.py:283\u001B[0m, in \u001B[0;36mModel.__init__\u001B[0;34m(self, patterns, encoding, **options)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatterns \u001B[38;5;241m=\u001B[39m patterns\n\u001B[0;32m--> 283\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model \u001B[38;5;241m=\u001B[39m \u001B[43m_wapiti\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi_new_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m    \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpointer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatterns\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mArgumentError\u001B[0m: argument 2: <class 'TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "Analyzer().info_of_text()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " دراین بخش با استفاده از پکیج هضم جمله را به توکن ها تقسیم میکنیم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Analyzer().tokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این بخش نقش دستوری کلمات در متن با استفاده از پکیج هضم مشخص میشود"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Analyzer().postagger()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این متد ما درصد کلمات به کار رفته در متن را تشخیص می دهیم که این به ما کمک میکند که بتوانیم راجع سهم موردنظر در آینده اطلاعات دقیق تری از قبیل تشخیص میزان رشد ، نزول ، نوسان سهام بگیریم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <class 'TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mArgumentError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mrate_keywords(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mسلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [33]\u001B[0m, in \u001B[0;36mAnalyzer.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msign \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m،\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m؟\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentence_tokenizer \u001B[38;5;241m=\u001B[39m SentenceTokenizer()\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_tagger \u001B[38;5;241m=\u001B[39m \u001B[43mPOSTagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/hazm/SequenceTagger.py:23\u001B[0m, in \u001B[0;36mSequenceTagger.__init__\u001B[0;34m(self, patterns, **options)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, patterns\u001B[38;5;241m=\u001B[39m[], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions):\n\u001B[1;32m     22\u001B[0m \t\u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwapiti\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[0;32m---> 23\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatterns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/wapiti/api.py:283\u001B[0m, in \u001B[0;36mModel.__init__\u001B[0;34m(self, patterns, encoding, **options)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatterns \u001B[38;5;241m=\u001B[39m patterns\n\u001B[0;32m--> 283\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model \u001B[38;5;241m=\u001B[39m \u001B[43m_wapiti\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi_new_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m    \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpointer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatterns\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mArgumentError\u001B[0m: argument 2: <class 'TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "Analyzer().rate_keywords('سلام دوستان من امروز ورود کردم نظزتون رو صادقانه بگید')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در آخر با استفاده از قواعد زبان منظم نام سهم ها و نماد ها را داخل متن تشخیص میدهیم"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def select_symbol_of_bourse(content: str):\n",
    "        symbols = {'وتجارت', 'وپارس', 'فملی', 'وبصادر', 'کترام', 'فاذر', 'شراز', 'چکاپا', 'آپ', 'لابسا', 'کاما',\n",
    "                   'پکویر', 'ثبهساز', 'کلر', 'پترول',\n",
    "                   'ورنا', 'لکما', 'بترانس', 'کسرا', 'خفنر', 'ذوب', 'خدیزل', 'شستا', 'شاراک', 'فولاد', 'شپترو',\n",
    "                   'وشهر',\n",
    "                   'قاسم', 'پالایش', 'خساپا', 'پاسا', 'فسبزوار', 'وبرق', 'غزر', 'سفار', 'نوری', 'زگلدشت', 'ولساپا',\n",
    "                   'وغدیر', 'سپید', 'وآیند', 'بکاب', 'وسالت', 'کیسون', 'تپکو', 'بجهرم', 'فروی', 'ختراک', 'همراه',\n",
    "                   'غبشهر',\n",
    "                   'غنوش', 'کیمیاتک', 'فلوله', 'تفارس-پذیره', 'آرام', 'خفولا', 'بالاس', 'غدشت', 'ثشاهد',\n",
    "                   'کتوکا', 'کفپارس', 'زماهان', 'شفن', 'دی', 'خپارس', 'غصینو', 'مادیرا', 'زاگرس', 'قچار', 'کرمان',\n",
    "                   'شکلر', 'شپلی', 'خکرمان', 'کدما', 'طلا', 'خنصیر', 'وهامونح', 'شلرد', 'برکت', 'کمند', 'وسین',\n",
    "                   'سجام',\n",
    "                   'مفاخر', 'شوینده', 'خکار', 'شیشه01ن', 'افق', 'شپدیس', 'خاور', 'تمحرکه', 'کالا', 'صبا', 'سیمرغ',\n",
    "                   'سمگا',\n",
    "                   'زگلدشتح', 'خکمک', 'فزرین', 'فنفت', 'رتاپ', 'دارا یکم', 'خگستر', 'وآذر', 'ساذری', 'خودکفا',\n",
    "                   'غالبر',\n",
    "                   'بزاگرس', 'غشهداب', 'وساپا', 'قنیشا', 'کگاز', 'فولای', 'وپست', 'خودرو', 'شگویا', 'خلنت', 'ثاخت',\n",
    "                   'شپنا', 'شتران', 'غگرجی', 'وبملت', 'سیتا', 'گشان', 'وگردش', 'وسدید'}\n",
    "\n",
    "        symbol_of_content = []\n",
    "        for symbol in symbols:\n",
    "            if symbol == 'دی':\n",
    "                if content.startswith('دی ') or content.endswith(' دی') or ' دی ' in content:\n",
    "                    symbol_of_content.append(symbol)\n",
    "                else:\n",
    "                    continue\n",
    "            if symbol in content:\n",
    "                symbol_of_content.append(symbol)\n",
    "\n",
    "        if len(symbol_of_content):\n",
    "            symbol_of_content = 'nothing'\n",
    "        return symbol_of_content"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " با وارد کردن متن مورد نظر در تابع زیر سهم های تشخیص داده شده در متن نمایش داده میشود"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_symbol_of_bourse()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " در این تمرین سعی شده خواسته های مورد نظر در داک تمرین به خوبی پیاده سازی شود\n",
    " با تشکر از حسن نظر شمااستاد گرامی"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
import re
from collections import Counter
from hazm import Normalizer, POSTagger, SentenceTokenizer




class Analyzer:
    def __init__(self):
        self.sign = {",", ".", "ØŒ", "?", "ØŸ", "!", "!", "#", "*", "(", ")", "[", "]", "{", "}", " "}
        # self.sentence_tokenizer = SentenceTokenizer()
        # self.pos_tagger = POSTagger()

    def mentions(self, text):
        return list(set(re.findall('@([\w_.]+)', text)))

    def __hashtags(self, text):
        return list(set(re.findall('#([\w_]+)', text)))

    def __numbers(self, text: str):
        number = ''
        numbers = []
        for letter in text:
            try:
                int(letter)
            except:
                if len(number) != 0:
                    numbers.append(number)
                    number = ''
                continue
            number += letter
        return numbers

    def __Letter_info(self, text: str):
        letters = []
        namads = []
        for letter in text:
            if letter not in self.sign:
                letters.append(letter)
            elif letter != " ":
                namads.append(letter)
        return {"letter_count": len(letters),
                "sign_count": len(namads)}

    def info_of_text(self, text: str):
        return {'word_count': self.__word_tokenizer(text)['word_count'],
                'numbers_info': {
                    'numbers': self.__numbers(text),
                    'count': len(self.__numbers(text))},
                'letters_count': self.__Letter_info(text),
                'hashtags_info': {
                    'items': self.__hashtags(text),
                    'count': len(self.__hashtags(text))},
                'mentions_info': {
                    'items': self.__mentions(text),
                    'count': len(self.__mentions(text))}
                }

    def tokenizer(self, text: str):
        return self.sentence_tokenizer.tokenize(text)

    def postagger(self, text):
        return self.pos_tagger.tag(text)

    def rate_keywords(self, text):
        keywords = {
            'Ø®Ø±ÛŒØ¯': ['Ø®Ø±ÛŒØ¯', 'Ø³ÙˆØ¯', 'Ø¨Ø§Ù„Ø§', 'Ø±Ø´Ø¯'],
            'ÙØ±ÙˆØ´': ['Ø¶Ø±Ø±', 'ÙØ±ÙˆØ´', 'Ù¾Ø§ÛŒÛŒÙ†', 'Ù†Ø²ÙˆÙ„'],
            'Ù†ÙˆØ³Ø§Ù†': ['Ù†ÙˆØ³Ø§Ù†', 'ØªØºÛŒÛŒØ±'],
        }
        rate = Counter(text.split())
        all = sum(dict(rate).values())
        info_rate = {}
        for word in rate:
            info_rate[word] = round(float((rate[word] / all) * 100), 2)
        print(info_rate)
        return info_rate



class SelfNormalizer:
    def __init__(self):
        self.normalizer = Normalizer

    def normalize(self, text: str) -> str:
        text = self.__remove_shapes_and_convert_emojis_to_unicode(text)
        text = self.__character_replacer(text)
        text = self.normalizer.normalize(text)
        text = self.__remove_stopword(text)

        return text.strip()

    def __remove_shapes_and_convert_emojis_to_unicode(self, text):
        shape_list = re.findall(r'[^\w\s,]', text)  # find all shape and emojis
        for shape in shape_list:
            shape_code = shape.encode('unicode-escape').decode('ASCII')
            if 'U000' in shape_code:  # if the shape is an emoji
                text = text.replace(shape, ' {} '.format(shape_code))
            else:
                text = text.replace(shape, ' ')
        return text

    def __character_replacer(self, text: str):
        # Numbers
        text = re.sub(r'[Ù â“ªâ“¿ï¼ğŸ¶ğŸ„Œ]', 'Û°', text)
        text = re.sub(r'[Ù¡â“µâ¶â€âŠê˜¡]', 'Û±', text)
        text = re.sub(r'[Ù¢â‘¡ï¼’ğŸ]', 'Û²', text)
        text = re.sub(r'[Ù£â‘¢ï¼“ğŸ›]', 'Û³', text)
        text = re.sub(r'[Ù¤Û´â“¸âğŸ’ğŸœ]', 'Û´', text)
        text = re.sub(r'[Ù¥â“¹âºï¼•ğŸ]', 'Ûµ', text)
        text = re.sub(r'[Ù¦Û¶â‘¥â»ï¼–ğŸğŸ¨]', 'Û¶', text)
        text = re.sub(r'[Ù§â†âï¼—ğŸŸ]', 'Û·', text)
        text = re.sub(r'[Ù¨â‘§â½ï¼˜ğŸ–]', 'Û¸', text)
        text = re.sub(r'[Ù©â‘¨â¾ğŸ—]', 'Û¹', text)

        # Alphabet
        text = re.sub(r'[Ø¢Ø£ğ¸€]', 'Ø§', text)
        text = re.sub(r'[Ø¨ï­’ï­“ï­”ï­•ğ¸]', 'Ø¨', text)
        text = re.sub(r'[ï­—ï­˜ï­™]', 'Ù¾', text)
        text = re.sub(r'[Øªïº–ï­§ğ¸•]', 'Øª', text)
        text = re.sub(r'[Ø«ïº™ïºšğ¸¶ğ¸–]', 'Ø«', text)
        text = re.sub(r'[ïºïºïº ğ¸¢ğ¸‚]', 'Ø¬', text)
        text = re.sub(r'[Ú†ï­»ï­¼ï®€]', 'Ú†', text)
        text = re.sub(r'[Ø­ïº¢ïº£ğ¸‡]', 'Ø­', text)
        text = re.sub(r'[ïº¦ïº¨ğ¸—]', 'Ø®', text)

        text = re.sub(r'[Ø¯ïº©ïºª]', 'Ø¯', text)
        text = re.sub(r'[Ø°ïº«ïº¬ğ¸˜]', 'Ø°', text)

        text = re.sub(r'[Ø±ïº­ïº®ğ¸“]', 'Ø±', text)
        text = re.sub(r'[Ø²à¢²ïº¯ïº°ğ¸†]', 'Ø²', text)
        text = re.sub(r'[Ú˜ï®Šï®‹]', 'Ú˜', text)
        text = re.sub(r'[ïº±ïº³ïº´ğ¸ğ¸®]', 'Ø³', text)
        text = re.sub(r'[ïºµïº¶ïº¸ğ¸´ğ¸”]', 'Ø´', text)
        text = re.sub(r'[Øµğ¸±ğ¸‘]', 'Øµ', text)
        text = re.sub(r'[Ø¶ï»€ğ¸¹ğ¸™]', 'Ø¶', text)
        text = re.sub(r'[ï»‚ï»ƒğ¸ˆ]', 'Ø·', text)
        text = re.sub(r'[ï»†ğ¸š]', 'Ø¸', text)
        text = re.sub(r'[Ø¹ï»‰ï»Šï»Œğ¸¯ğ¸]', 'Ø¹', text)
        text = re.sub(r'[ï»ï»ï»ğ¸»ğ¸›]', 'Øº', text)
        text = re.sub(r'[Ùğ¸ğ¸]', 'Ù', text)
        text = re.sub(r'[ï»–ï»˜ğ¸Ÿğ¸’]', 'Ù‚', text)
        text = re.sub(r'[Ú¯ï®“ï®”ï®•]', 'Ú¯', text)
        text = re.sub(r'[Ùƒï®‘ğ¸Šğ¸ª]', 'Ú©', text)
        text = re.sub(r'[ï»ï»ï»Ÿğ¸‹]', 'Ù„', text)
        text = re.sub(r'[Ù…ï»¡ï»¤ğ¸¬ğ¸Œ]', 'Ù…', text)
        text = re.sub(r'[ï»¥ğ¸ğ¸­]', 'Ù†', text)
        text = re.sub(r'[ï»­ï»®ğ¸…Û…ï¯ ]', 'Ùˆ', text)
        text = text.replace('ÙˆÙˆ', 'Ùˆ')
        text = re.sub(r'[Ù‡ï®ªï»ªğ¸¤ï»«ï»¬]', 'Ù‡', text)
        text = re.sub(r'[Ø©ïº”]', 'Ù‡', text)
        text = re.sub(r'[Ù‰ï»¯ï»°ğ¸‰ï¯¨ï¯©]', 'ÛŒ', text)
        text = text.replace('Ø¦ÛŒ', 'ÛŒÛŒ')
        text = re.sub(r'[Ø¦ïº‰ïº‹]', 'Ø¦', text)
        text = re.sub(r'[Ø¡ïº€Û½]', 'Ø¡', text)
        text = text.replace('ï·¼', ' Ø±ÛŒØ§Ù„ ')

        text = text.replace(' Ù…ÛŒ ', ' Ù…ÛŒ\u200c')
        text = text.replace(' Ù†Ù…ÛŒ ', ' Ù†Ù…ÛŒ\u200c')
        text = text.replace(' Ø¨Ø±Ù…ÛŒ ', ' Ø¨Ø±Ù…ÛŒ\u200c')
        text = text.replace(' Ø¨Ø±Ù†Ù…ÛŒ ', ' Ø¨Ø±Ù†Ù…ÛŒ\u200c')

        # Whitespace
        text = re.sub(r'(\r|\f|\v|\\r|\\n)+', '\n', text)
        text = re.sub(r'\s?\n\s+', '\n', text)
        text = re.sub(r'[\t]+', ' ', text)
        text = re.sub(r' {2,}', ' ', text)

        # semi-space
        text = text.replace('&zwnj;', '\u200c')
        text = re.sub(r'[\u2000-\u200f]+', "\u200c", text)
        return text

    def __remove_stopword(self, text):
        stop_words = ['Ø§Ø²', 'Ø¨Ù‡ ', 'Ø¨Ø§', 'Ù†Ù‡ ', 'Ø±Ø§', 'Ú©Ù‡ ', 'Ø¨ÙˆØ¯', 'Ø§Ø³Øª', 'Ù‡Ø³Øª', 'Ø´Ø¯', ' Ø¯Ø± ', 'Ø§Ú¯Ø± ', 'Ù‡Ù…Ú†Ù†ÛŒÙ† ',
                      'Ú†Ù†ÛŒÙ† ', 'Ø¯Ø§Ø´Øª']
        for word in stop_words:
            text = re.sub(word, ' ', text)
        return text


if __name__ == '__main__':
    print(Analyzer().mentions('Ø¯ÛŒ Ø§Ù…Ø±ÙˆØ² Ù¾Ù†Ø¬ Ù¾Ù†Ø¬ Ø¯Ø±ØµØ¯ Ø±Ø´Ø¯ Ø¯Ø§Ø´Øª'))

import re


class Analyzer:
    def __init__(self):
        self.namad = {",", ".", "ØŒ", "?", "ØŸ", "!", "!", "#", "*", "(", ")", "[", "]", "{", "}", " "}

    def __word_tokenizer(self, text: str):
        word = ''
        words = []
        for letter in text:
            try:
                int(letter)
                if len(word) != 0:
                    words.append(word)
                    word = ''
                continue
            except:
                if letter in self.namad:
                    if len(word) != 0:
                        words.append(word)
                        word = ''
                    continue
                word += letter
        return {'words': words,
                'word_count': len(words)}

    def __mention_count(self, text):
        return list(set(re.findall('@([\w_.]+)', text)))

    def __hashtag_count(self, text):
        return list(set(re.findall('#([\w_]+)', text)))

    def __number_count(self, text: str):
        number = ''
        numbers = []
        for letter in text:
            try:
                int(letter)
            except:
                if len(number) != 0:
                    numbers.append(number)
                    number = ''
                continue
            number += letter
        return len(numbers)

    def __Letter_info(self, text: str):
        letters = []
        namads = []
        for letter in text:
            if letter not in self.namad:
                letters.append(letter)
            elif letter != " ":
                namads.append(letter)
        return {"letter_count": len(letters),
                "namad_count": len(namads)}

    def info_of_text(self, text: str):
        return {'word_count': self.__word_tokenizer(text)['word_count'],
                'number_count': self.__number_count(text),
                'letter_count': self.__Letter_info(text),
                'hashtag_count': len(self.__hashtag_count(text)),
                'mention_count': len(self.__mention_count(text))
                }

    def tokenizer(self, text: str):
        return self.__word_tokenizer(text)['words']


class Normalizer:
    def normalize(self, text: str) -> str:
        text = self.__remove_shapes_and_convert_emojis_to_unicode(text)
        text = self.__character_replacer(text)

        return text.strip()

    def __remove_shapes_and_convert_emojis_to_unicode(self, text):
        shape_list = re.findall(r'[^\w\s,]', text)  # find all shape and emojis
        for shape in shape_list:
            shape_code = shape.encode('unicode-escape').decode('ASCII')
            if 'U000' in shape_code:  # if the shape is an emoji
                text = text.replace(shape, ' {} '.format(shape_code))
            else:
                text = text.replace(shape, ' ')
        return text

    def __character_replacer(self, text: str):
        # Numbers
        text = re.sub(r'[Ù â“ªâ“¿ï¼ğŸ¶ğŸ„Œ]', 'Û°', text)
        text = re.sub(r'[Ù¡â“µâ¶â€âŠê˜¡]', 'Û±', text)
        text = re.sub(r'[Ù¢â‘¡ï¼’ğŸ]', 'Û²', text)
        text = re.sub(r'[Ù£â‘¢ï¼“ğŸ›]', 'Û³', text)
        text = re.sub(r'[Ù¤Û´â“¸âğŸ’ğŸœ]', 'Û´', text)
        text = re.sub(r'[Ù¥â“¹âºï¼•ğŸ]', 'Ûµ', text)
        text = re.sub(r'[Ù¦Û¶â‘¥â»ï¼–ğŸğŸ¨]', 'Û¶', text)
        text = re.sub(r'[Ù§â†âï¼—ğŸŸ]', 'Û·', text)
        text = re.sub(r'[Ù¨â‘§â½ï¼˜ğŸ–]', 'Û¸', text)
        text = re.sub(r'[Ù©â‘¨â¾ğŸ—]', 'Û¹', text)

        # Alphabet
        text = re.sub(r'[Ø¢Ø£ğ¸€]', 'Ø§', text)
        text = re.sub(r'[Ø¨ï­’ï­“ï­”ï­•ğ¸]', 'Ø¨', text)
        text = re.sub(r'[ï­—ï­˜ï­™]', 'Ù¾', text)
        text = re.sub(r'[Øªïº–ï­§ğ¸•]', 'Øª', text)
        text = re.sub(r'[Ø«ïº™ïºšğ¸¶ğ¸–]', 'Ø«', text)
        text = re.sub(r'[ïºïºïº ğ¸¢ğ¸‚]', 'Ø¬', text)
        text = re.sub(r'[Ú†ï­»ï­¼ï®€]', 'Ú†', text)
        text = re.sub(r'[Ø­ïº¢ïº£ğ¸‡]', 'Ø­', text)
        text = re.sub(r'[ïº¦ïº¨ğ¸—]', 'Ø®', text)

        text = re.sub(r'[Ø¯ïº©ïºª]', 'Ø¯', text)
        text = re.sub(r'[Ø°ïº«ïº¬ğ¸˜]', 'Ø°', text)
        text = re.sub(r'[Ø±ïº­ïº®ğ¸“]', 'Ø±', text)
        text = re.sub(r'[Ø²à¢²ïº¯ïº°ğ¸†]', 'Ø²', text)
        text = re.sub(r'[Ú˜ï®Šï®‹]', 'Ú˜', text)
        text = re.sub(r'[ïº±ïº³ïº´ğ¸ğ¸®]', 'Ø³', text)
        text = re.sub(r'[ïºµïº¶ïº¸ğ¸´ğ¸”]', 'Ø´', text)
        text = re.sub(r'[Øµğ¸±ğ¸‘]', 'Øµ', text)
        text = re.sub(r'[Ø¶ï»€ğ¸¹ğ¸™]', 'Ø¶', text)
        text = re.sub(r'[ï»‚ï»ƒğ¸ˆ]', 'Ø·', text)
        text = re.sub(r'[ï»†ğ¸š]', 'Ø¸', text)
        text = re.sub(r'[Ø¹ï»‰ï»Šï»Œğ¸¯ğ¸]', 'Ø¹', text)
        text = re.sub(r'[ï»ï»ï»ğ¸»ğ¸›]', 'Øº', text)
        text = re.sub(r'[Ùğ¸ğ¸]', 'Ù', text)
        text = re.sub(r'[ï»–ï»˜ğ¸Ÿğ¸’]', 'Ù‚', text)
        text = re.sub(r'[Ú¯ï®“ï®”ï®•]', 'Ú¯', text)
        text = re.sub(r'[Ùƒï®‘ğ¸Šğ¸ª]', 'Ú©', text)
        text = re.sub(r'[ï»ï»ï»Ÿğ¸‹]', 'Ù„', text)
        text = re.sub(r'[Ù…ï»¡ï»¤ğ¸¬ğ¸Œ]', 'Ù…', text)
        text = re.sub(r'[ï»¥ğ¸ğ¸­]', 'Ù†', text)
        text = re.sub(r'[ï»­ï»®ğ¸…Û…ï¯ ]', 'Ùˆ', text)
        text = text.replace('ÙˆÙˆ', 'Ùˆ')
        text = re.sub(r'[Ù‡ï®ªï»ªğ¸¤ï»«ï»¬]', 'Ù‡', text)
        text = re.sub(r'[Ø©ïº”]', 'Ù‡', text)
        text = re.sub(r'[Ù‰ï»¯ï»°ğ¸‰ï¯¨ï¯©]', 'ÛŒ', text)
        text = text.replace('Ø¦ÛŒ', 'ÛŒÛŒ')
        text = re.sub(r'[Ø¦ïº‰ïº‹]', 'Ø¦', text)
        text = re.sub(r'[Ø¡ïº€Û½]', 'Ø¡', text)
        text = text.replace('ï·¼', ' Ø±ÛŒØ§Ù„ ')

        text = text.replace(' Ù…ÛŒ ', ' Ù…ÛŒ\u200c')
        text = text.replace(' Ù†Ù…ÛŒ ', ' Ù†Ù…ÛŒ\u200c')
        text = text.replace(' Ø¨Ø±Ù…ÛŒ ', ' Ø¨Ø±Ù…ÛŒ\u200c')
        text = text.replace(' Ø¨Ø±Ù†Ù…ÛŒ ', ' Ø¨Ø±Ù†Ù…ÛŒ\u200c')

        # Whitespace
        text = re.sub(r'(\r|\f|\v|\\r|\\n)+', '\n', text)
        text = re.sub(r'\s?\n\s+', '\n', text)
        text = re.sub(r'[\t]+', ' ', text)
        text = re.sub(r' {2,}', ' ', text)

        # semi-space
        text = text.replace('&zwnj;', '\u200c')
        text = re.sub(r'[\u2000-\u200f]+', "\u200c", text)

        return text
